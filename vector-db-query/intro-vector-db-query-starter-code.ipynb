{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a61385",
   "metadata": {},
   "source": [
    "![intro-to-vector-databases-queries](../assets/intro-vector-db-query.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e5c130",
   "metadata": {},
   "source": [
    "### Learning objective:\n",
    "By the end of this lesson, students will be able to load, split, and embed documents to store and query witihin a vector database. \n",
    " \n",
    "\n",
    "### About:  \n",
    "In this lesson, we introduce the concept of vector databases and apply them to question and answer chains. \n",
    "\n",
    "### Prerequisites:\n",
    "- Introduction to LangChain\n",
    "- Advanced Prompt Development Module\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf4e5f0",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. [Imports](#imports)\n",
    "1. [Vector Database Workflow](#vec-db)\n",
    "    1. [Load Documents](#load-docs)\n",
    "    1. [Split Documents](#split-docs)\n",
    "    1. [Embed and Store Documents](#embed-docs)\n",
    "    1. [Vector Database Queries](#query) \n",
    "\n",
    "### Activities\n",
    "1. [Try it!](#try-it)\n",
    "1. [Lab](#lab)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "- [Chroma](https://www.trychroma.com/)\n",
    "- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b9ed1e",
   "metadata": {},
   "source": [
    "### Installs\n",
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1cd791",
   "metadata": {},
   "source": [
    "<a id='vec-db'></a>\n",
    "## Vector Databases \n",
    "Vector databases are a way to store and query text embeddings from documents. \n",
    "- Build models using custom docs\n",
    "- Can be computationally expensive depending on the volume of your data\n",
    "\n",
    "\n",
    "### Key Steps\n",
    "1. Load raw text data from a source (e.g., websites, articles, etc.) \n",
    "2. Split docs transforming the text into documents for the language model \n",
    "3. Embed the documents \n",
    "    - As a reminder, text embeddings are numerical representation of the how free text is used in your documents\n",
    "    - You can use a 3rd party algorithm  to create your text embeddings (available through Open AI, Hugging Face, Cohere, etc.) and create them using ```.embed_documents``` in [LangChain](https://python.langchain.com/docs/modules/data_connection/text_embedding/)\n",
    "    - We will combine this step with creating the vector database using Chroma \n",
    "4. Store the embeddings in a vector database/vector store\n",
    "    - You can use a 3rd party database along with LangChain tooling for this purpose including : \n",
    "        - Chroma - what we will use\n",
    "        - FAISS\n",
    "        - Lance\n",
    "        - [source](https://python.langchain.com/docs/modules/data_connection/vectorstores/)\n",
    "5. Query the vector store as needed \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd204e5",
   "metadata": {},
   "source": [
    "<a id='imports'></a>\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f83d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI #openai chatbot\n",
    "from langchain_core.prompts import ChatPromptTemplate #template for chat prompts\n",
    "from langchain_core.output_parsers import StrOutputParser #output parser for string output \n",
    "\n",
    "# loaders and splitters \n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "#new imports \n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb16fae0",
   "metadata": {},
   "source": [
    "<a id='load-docs'></a>\n",
    "### Load Our Docs \n",
    "We will load text from the wikipedia ancient Rome article [source](https://en.wikipedia.org/wiki/Ancient_Rome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0ba162",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Ancient_Rome\")\n",
    "pages = loader.load()\n",
    "print(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99f920",
   "metadata": {},
   "source": [
    "<a id='split-docs'></a>\n",
    "### Split Our Text and Docs \n",
    "Now we will split our text into smaller chunks for the processing by the language model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca33db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "split_docs = text_splitter.split_documents(pages)\n",
    "print(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ea644",
   "metadata": {},
   "source": [
    "<a id='embed-docs'></a>\n",
    "### Create Text Embeddings and Store in Chroma Vector Database\n",
    "Now we will create text embeddings using OpenAI embeddings through LangChain and will store them in [Chroma].(https://www.trychroma.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8eee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(split_docs, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9249ee",
   "metadata": {},
   "source": [
    "<a id='query'></a>\n",
    "### Query Database\n",
    "\n",
    "There are variety of search methods that we can use to query our vector database. Similarity search is common and we will use it in our lab today with the ```.similarity_search()``` method. \n",
    "\n",
    "#### Query Methods\n",
    "1. [Similarity Search](https://python.langchain.com/docs/modules/data_connection/vectorstores/#similarity-search)\n",
    "1. [Maximum Marginal Relevance Retrieval (MMR) ](https://python.langchain.com/docs/modules/data_connection/vectorstores/#maximum-marginal-relevance-search-mmr)\n",
    "1. [Top K](https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore#specifying-top-k) \n",
    "1. Additional info: [Link](https://python.langchain.com/docs/modules/data_connection/vectorstores/#similarity-search-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eef7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What structures were built in ancient Rome?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b8508a",
   "metadata": {},
   "source": [
    "<a id='try-it'></a>\n",
    "### Try it\n",
    "Try writing your own query about ancient Rome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f36246b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc816f2b",
   "metadata": {},
   "source": [
    "<a id='lab'></a>\n",
    "## Lab\n",
    "We will use the meetings notes stored in my_docs. These notes were generated by GPT, and will be loaded using the the file directory loader in LangChain. [LangChain Docs](https://python.langchain.com/docs/modules/data_connection/document_loaders/file_directory)\n",
    "\n",
    "Objectives:\n",
    "\n",
    "1. Load my_documents from file folder, split, embed, and store in a Chroma vector database\n",
    "1. Use a similarity search to find how revenue was discussed our prior meetings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da8d79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1ebf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split with recursive character text splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0197b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector database called db_notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dba683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity Search Query \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1501dce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
