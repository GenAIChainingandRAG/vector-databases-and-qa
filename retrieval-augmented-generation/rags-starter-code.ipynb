{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a61385",
   "metadata": {},
   "source": [
    "![RAGs](../assets/RAGs.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e5c130",
   "metadata": {},
   "source": [
    "### Learning Objective:\n",
    "By the end of the lesson, students will be able to describe common use case for Retrieval augmented generation (RAGs) and apply to a quesiton and answer system. \n",
    " \n",
    "\n",
    "### About:  \n",
    "LLM are trained on large general databases that are typically behind 6-8 months or more. Often we want to apply the power of LLM to our own data, such as private company docs or more recent information (such as news). Retrieval Augmented Generation or RAGs make this possible.  \n",
    "\n",
    "### Prerequisites:\n",
    "- Intro to Vector Databases and Queries \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf4e5f0",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. [Imports](#imports)\n",
    "1. [Load, Split, Embed, Store](#vec-db)\n",
    "1. [Retrieve](#retrieve) \n",
    "1. [Generate](#generate)\n",
    "\n",
    "### Activities\n",
    "1. [Q&A Lab](#lab)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "- [Chroma](https://www.trychroma.com/)\n",
    "- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)\n",
    "- [RAGs](https://python.langchain.com/docs/use_cases/question_answering/)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b9ed1e",
   "metadata": {},
   "source": [
    "### Installs\n",
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd204e5",
   "metadata": {},
   "source": [
    "<a id='imports'></a>\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f83d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI #openai chatbot\n",
    "from langchain_core.prompts import ChatPromptTemplate #template for chat prompts\n",
    "from langchain_core.output_parsers import StrOutputParser #output parser for string output \n",
    "\n",
    "# loaders and splitters \n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "#vector store and embeddings \n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# new! runnables for q&A \n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1cd791",
   "metadata": {},
   "source": [
    "<a id='vec-db'></a>\n",
    "\n",
    "## Intro to RAGs\n",
    "LLM are trained on large general databases that are typically behind 6-8 months or more. Often we want to apply the power of LLM to our own data, such as private company docs or more recent information (such as news). Retrieval Augmented Generation, or RAGs, make this possible.  \n",
    "\n",
    "## Load, Split, Embed and Store\n",
    "In the Introduction to Vector Databases and Queries lesson, we explored how to load, split, embed and store documents in a vector databases. Additionally, we did a simple similarity query. [source](https://python.langchain.com/docs/modules/data_connection/vectorstores/)\n",
    "\n",
    "\n",
    "## Retrieve and Generate \n",
    "#### For RAGs, we will add two new steps: \n",
    "1. [Retrieve](#retrieve): Retrieve is similar to a query. This step pulls documents to share with the LLM in the next step.\n",
    "1. [Generate](#generate): Where we pass a prompt and documentation to the LLM \n",
    "\n",
    "Notice that this is similar to how Github Copilot works. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb16fae0",
   "metadata": {},
   "source": [
    "<a id='load-docs'></a>\n",
    "#### Load, Split, Embed and Store Our Docs \n",
    "We will load and process text from the Wikipedia [Ancient Rome article](https://en.wikipedia.org/wiki/Ancient_Rome) storing its embeddings in a vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0ba162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load \n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Ancient_Rome\")\n",
    "pages = loader.load()\n",
    "\n",
    "#split\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "split_docs = text_splitter.split_documents(pages)\n",
    "\n",
    "# embed and store\n",
    "db = Chroma.from_documents(split_docs, OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b7a13",
   "metadata": {},
   "source": [
    "<a id='retrieve'></a>\n",
    "### Retrieve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8eee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve docs using a similarity search query and save as retrieved_docs\n",
    "retriever = db.as_retriever(search_type=\"similarity\")\n",
    "retrieved_docs = retriever.invoke(\"What structures were built in ancient Rome?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore results \n",
    "print(\"Number of retrieved docs:\", len(retrieved_docs))\n",
    "print(retrieved_docs[0].page_content) # first doc content \n",
    "print(retrieved_docs[-1].page_content) # last doc content "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1793183f",
   "metadata": {},
   "source": [
    "<a id='generate'></a>\n",
    "### Generate\n",
    "\n",
    "Now we will take our retrieved documents and combine them with a user prompt and pass all of that to a language model. Here we will build a standard prompt chain with a few modifications to also combine and pass our documents to the model as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a9d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Model \n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Prompt\n",
    "prompt_txt= \"\"\"Answer the question based only on the following context: {context}\n",
    "                Question: {question}\n",
    "            \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_txt)\n",
    "\n",
    "# Create RAG Chain \n",
    "rag_chain = ( {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aea7ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke\n",
    "rag_chain.invoke(\"What was built in ancient Rome?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88616716",
   "metadata": {},
   "source": [
    "### You can look under the hood a bit with debug\n",
    "\n",
    "#### Example to show how it is passed to our language model: \n",
    "``` \n",
    "\"prompts\": [\n",
    "    \"Human: Answer the question based only on the following context: \n",
    "    \n",
    "[Document(page_content=\\\"Culture[edit]\\\\nMain article: Culture of ancient Rome\\\\nThe seven hills of Rome\\\\nLife in ancient Rome revolved around the city of Rome, located on seven hills. The city had a vast number of monumental structures like the Colosseum, the Trajan's Forum and the Pantheon. It had theatres, gymnasiums, marketplaces, functional sewers, bath complexes complete with libraries and shops, and fountains with fresh drinking water supplied by hundreds of miles of aqueducts. Throughout the territory under the control of ancient Rome, residential architecture ranged from modest houses to country villas.\\\", metadata={'language': 'en', 'source': 'https://en.wikipedia.org/wiki/Ancient_Rome', 'title': 'Ancient Rome - Wikipedia'}), Document(page_content='The Romans were renowned for their architecture, which is grouped with Greek traditions into \\\"Classical architecture\\\". Although there were many differences from Greek architecture, Rome borrowed heavily from Greece in adhering to strict, formulaic building designs and proportions. Aside from two new orders of columns, composite and Tuscan, and from the dome, which was derived from the Etruscan arch, Rome had relatively few architectural innovations until the end of the Republic.', metadata={'language': 'en', 'source': 'https://en.wikipedia.org/wiki/Ancient_Rome', 'title': 'Ancient Rome - Wikipedia'}), Document(page_content=\\\"Technology[edit]\\\\nMain article: Ancient Roman technology\\\\nPont du Gard in France is a Roman aqueduct built in c.\\\\xa019\\\\xa0BC. It is a World Heritage Site.\\\\nAncient Rome boasted impressive technological feats, using many advancements that were lost in the Middle Ages and not rivalled again until the 19th and 20th centuries. An example of this is insulated glazing, which was not invented again until the 1930s. Many practical Roman innovations were adopted from earlier Greek designs. Advancements were often divided and based on craft. Artisans guarded technologies as trade secrets.[223]\\\\nRoman civil engineering and military engineering constituted a large part of Rome's technological superiority and legacy, and contributed to the construction of hundreds of roads, bridges, aqueducts, public baths, theatres and arenas. Many monuments, such as the Colosseum, Pont du Gard, and Pantheon, remain as testaments to Roman engineering and culture.\\\", metadata={'language': 'en', 'source': 'https://en.wikipedia.org/wiki/Ancient_Rome', 'title': 'Ancient Rome - Wikipedia'}), Document(page_content=\\\"Archaeological evidence of settlement around Rome starts to emerge c.\\\\u20091000\\\\xa0BC.[4] Large-scale organisation appears only c.\\\\u2009800\\\\xa0BC, with the first graves in the Esquiline Hill's necropolis, along with a clay and timber wall on the bottom of the Palatine Hill dating to the middle of the 8th century\\\\xa0BC. Starting from c.\\\\u2009650\\\\xa0BC, the Romans started to drain the valley between the Capitoline and Palatine Hills, where today sits the Roman Forum.[5] By the sixth century, the Romans were constructing the Temple of Jupiter Optimus Maximus on the Capitoline and expanding to the Forum Boarium located between the Capitoline and Aventine Hills.[6]\\\", metadata={'language': 'en', 'source': 'https://en.wikipedia.org/wiki/Ancient_Rome', 'title': 'Ancient Rome - Wikipedia'})]\\n                \n",
    "\n",
    "\n",
    "Question: What was built in ancient Rome?\"\n",
    "  ]\n",
    " ```\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ddc1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.globals import set_debug\n",
    "# set_debug(True)\n",
    "# rag_chain.invoke(\"What was built in ancient Rome?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc816f2b",
   "metadata": {},
   "source": [
    "<a id='lab'></a>\n",
    "# Q&A Lab\n",
    "We will use the meetings notes stored in `my_docs`. These notes were generated by GPT, and will be loaded, split, embedded, and stored as before. We will then pass our docs along with our question to the language model.\n",
    "\n",
    "**Objectives:**\n",
    "1. Load `my_documents` from file folder, split, embed, and store in a Chroma vector database.\n",
    "1. Build a RAG chain that searches for docs based on a similarity search and returns relevant docs, then passes your retrevied docs as context along with a user prompt to an LLM, and returns the result. \n",
    "1. Bonus! Compare result to a basic similarity search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200feb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da8d79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load \n",
    "\n",
    "\n",
    "#split \n",
    "\n",
    "# create a vector database called db_notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496102ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dba683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with RAG Chain  \n",
    "\n",
    "# Specify Model \n",
    "\n",
    "\n",
    "# Write Prompt\n",
    "\n",
    "\n",
    "# Create RAG Chain \n",
    "\n",
    "# invoke chain with the user question (feel free to edit the question! )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cba1f9e",
   "metadata": {},
   "source": [
    "### Bonus: Compare\n",
    "\n",
    "Notice the difference in the retrieved docs and user question being passed to a language model (above) vs. a basic similarity search (below) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa8ab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # similarity search comparison- Similarity Search Query \n",
    "# query = \"What was discussed about revenue?\"\n",
    "# notes_docs= db_notes.similarity_search(query)\n",
    "# print(notes_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1efc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
